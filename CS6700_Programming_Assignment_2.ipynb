{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "!pip install gym[classic_control]"
      ],
      "metadata": {
        "id": "til9dnk_77WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jg9HCHqLrhw-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "# from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import deque, namedtuple\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "no_of_actions = env.action_space.n\n",
        "\n",
        "print(state_shape)\n",
        "print(no_of_actions)\n",
        "print(env.action_space.sample())"
      ],
      "metadata": {
        "id": "vIVj2qxX-VnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dueling DQN\n"
      ],
      "metadata": {
        "id": "EGRfppTI4v24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "LR = 1e-3              # learning rate\n",
        "UPDATE_EVERY = 20       # how often to update the network (When Q target is present)\n",
        "\n",
        "\n",
        "class QNetwork1(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=256, typ = 1): #typ is for type of update equation to be used in forward method\n",
        "\n",
        "        super(QNetwork1, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "\n",
        "        self.value_fc = nn.Linear(fc1_units, fc2_units)\n",
        "        self.adv_fc = nn.Linear(fc1_units, fc2_units)\n",
        "\n",
        "        self.value = nn.Linear(fc2_units, 1)\n",
        "        self.adv = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "        assert typ == 1 or typ == 2, \"Type should either be 1 or 2\"\n",
        "        self.typ = typ\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        # x = F.relu(self.fc2(x))\n",
        "        val = F.relu(self.value_fc(x))\n",
        "        adv = F.relu(self.adv_fc(x))\n",
        "\n",
        "        val = self.value(val)\n",
        "        adv = self.adv(adv)\n",
        "\n",
        "        if self.typ  == 1 :\n",
        "            return val + adv - torch.mean(adv, dim = 1, keepdim=True)\n",
        "        else :\n",
        "            return val + adv - torch.max(adv, dim = 1,  keepdim=True)"
      ],
      "metadata": {
        "id": "QPjK02wVj5Mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "QpfyoUAoBAh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, typ = 1):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.qnetwork_local = QNetwork1(state_size = state_size, action_size= action_size, seed= seed, typ = typ).to(device)\n",
        "        self.qnetwork_target = QNetwork1(state_size = state_size, action_size= action_size, seed= seed, typ = typ).to(device)\n",
        "        self.optimizer = torch.optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        if len(self.memory) >= BATCH_SIZE:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "\n",
        "            self.qnetwork_target.load_state_dict(self.qnetwork_local.state_dict())\n",
        "\n",
        "    def act(self, state, eps=0.01):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        with torch.no_grad():\n",
        "            Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "XcSOZbhSBU3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dueling_dqn(env, agent, typ, avg_window_score, n_episodes=10000, max_t=1000, eps_start=0.3, eps_end=0.01, eps_decay=0.995):\n",
        "    max_len = 100\n",
        "    scores_window = deque(maxlen=max_len)\n",
        "\n",
        "    eps = eps_start\n",
        "    rewards = np.zeros((n_episodes,))\n",
        "    for i_episode in (range(1, n_episodes+1)):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "\n",
        "        rewards[i_episode-1] = score\n",
        "        # rewards[i_episode-1] = score\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=avg_window_score:\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "    return agent, np.array(rewards)\n"
      ],
      "metadata": {
        "id": "hfAoziZaCKJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CARTPOLE"
      ],
      "metadata": {
        "id": "d1pHGcz63dh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 5\n",
        "episodes = 10000\n",
        "rewards_avg = np.zeros((runs, episodes,))\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "for run in range(runs) :\n",
        "  s = np.random.randint(0, 1000)\n",
        "  print(\"-\"*45)\n",
        "  print(f\"Running experiment with seed value as {s}\")\n",
        "  print(\"-\"*45)\n",
        "  agent = Agent(state_size=state_shape,action_size = action_shape, seed = s)\n",
        "  agent, rewards = dueling_dqn(env, agent,  n_episodes = episodes, typ = 1, avg_window_score = 195)\n",
        "  rewards_avg[run] = rewards"
      ],
      "metadata": {
        "id": "6wzMsJ6hSKv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_run = {}\n",
        "cartpole_run[\"type1\"] = rewards_avg"
      ],
      "metadata": {
        "id": "w1h_v2P-xnhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 5\n",
        "rewards_avg = np.zeros((runs, 10000,))\n",
        "for run in range(runs) :\n",
        "  s = np.random.randint(0, 1000)\n",
        "  print(\"-\"*45)\n",
        "  print(f\"Running experiment with seed value as {s}\")\n",
        "  print(\"-\"*45)\n",
        "  agent = Agent(state_size=state_shape,action_size = action_shape, seed = s)\n",
        "  agent, rewards = dueling_dqn(env, agent, typ = 2, avg_window_score=195)\n",
        "  rewards_avg[run] = rewards"
      ],
      "metadata": {
        "id": "j_ibKwNDSNQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_run[\"type2\"] = rewards_avg"
      ],
      "metadata": {
        "id": "WrpkNWu4yIUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_rewards_episodes(steps, message = \"Rewards vs Episodes\"):\n",
        "\n",
        "  fig, ax =  plt.subplots()\n",
        "  for step in steps:\n",
        "    mini = np.mean(step, axis=0) - np.std(step, axis = 0)\n",
        "    maxi = np.mean(step, axis=0) + np.std(step, axis = 0)\n",
        "    ax.plot(np.arange(len(step[0])), np.mean(step, axis=0))\n",
        "    ax.fill_between(np.arange(len(step[0])), maxi, mini, alpha = 0.3)\n",
        "  ax.set_xlabel('Episode')\n",
        "  ax.set_ylabel('Average of Total Rewards')\n",
        "  fig.suptitle(message)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_rewards_episodes([cartpole_run[\"type1\"][:, :100]], \"Episode Rewards vs Episode\")"
      ],
      "metadata": {
        "id": "Lc5vgUqBJThG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards_episodes([cartpole_run[\"type2\"][:, :100]], \"Episode Rewards vs Episode\")"
      ],
      "metadata": {
        "id": "-7jov_JT36FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards_episodes([cartpole_run[\"type1\"][:, :100], cartpole_run[\"type2\"][:, :100]], \"Episode Rewards vs Episode\")"
      ],
      "metadata": {
        "id": "htEhp13J4owS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ACROBOT"
      ],
      "metadata": {
        "id": "322vQpSg3ZbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Acrobot-v1')\n",
        "env.seed(0)\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "no_of_actions = env.action_space.n\n",
        "\n",
        "print(state_shape)\n",
        "print(no_of_actions)\n",
        "print(env.action_space.sample())"
      ],
      "metadata": {
        "id": "i9gOwCwFc-Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 5\n",
        "episodes = 10000\n",
        "rewards_avg = np.zeros((runs, episodes,))\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "LR = 3e-3\n",
        "for run in range(runs) :\n",
        "  s = np.random.randint(0, 1000)\n",
        "  print(\"-\"*45)\n",
        "  print(f\"Running experiment with seed value as {s}\")\n",
        "  print(\"-\"*45)\n",
        "  agent = Agent(state_size=state_shape,action_size = action_shape, seed = s)\n",
        "  agent, rewards = dueling_dqn(env, agent,  n_episodes = episodes, typ = 1, avg_window_score = -80)\n",
        "  rewards_avg[run] = rewards"
      ],
      "metadata": {
        "id": "PTS9lOvl50KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acrobot_run = {}\n",
        "acrobot_run[\"type1\"] = rewards_avg"
      ],
      "metadata": {
        "id": "EyQal3LOJqf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 5\n",
        "episodes = 10000\n",
        "rewards_avg = np.zeros((runs, episodes,))\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "LR = 3e-3\n",
        "for run in range(runs) :\n",
        "  s = np.random.randint(0, 1000)\n",
        "  print(\"-\"*45)\n",
        "  print(f\"Running experiment with seed value as {s}\")\n",
        "  print(\"-\"*45)\n",
        "  agent = Agent(state_size=state_shape,action_size = action_shape, seed = s)\n",
        "  agent, rewards = dueling_dqn(env, agent,  n_episodes = episodes, typ = 2, avg_window_score = -80)\n",
        "  rewards_avg[run] = rewards"
      ],
      "metadata": {
        "id": "pzB2AQfaMmeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acrobot_run[\"type2\"] = rewards_avg"
      ],
      "metadata": {
        "id": "kMa1J_VNM_PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards_episodes([acrobot_run[\"type1\"][:, :600]], \"Episode Rewards vs Episode\")"
      ],
      "metadata": {
        "id": "cQ1jZkTOXNtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards_episodes([acrobot_run[\"type2\"][:, :580]], \"Episode Rewards vs Episode\")"
      ],
      "metadata": {
        "id": "sYl8D_cBXdhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards_episodes([acrobot_run[\"type1\"][:, :580], acrobot_run[\"type2\"][:, :580]], \"Episode Rewards vs Episode\")"
      ],
      "metadata": {
        "id": "6-MLCXlvluB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Monte Carlo REINFORCE"
      ],
      "metadata": {
        "id": "War5INzQ6jnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "LR = 1e-3              # learning rate"
      ],
      "metadata": {
        "id": "zt92Xr8FqS5l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "import gym\n",
        "from tqdm import tqdm_notebook\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=256):\n",
        "\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.softmax(self.fc3(x), dim = 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class StateValueNetwork(nn.Module) :\n",
        "    def __init__(self, state_size, seed, fc1_units = 64, fc2_units = 128):\n",
        "\n",
        "        super(StateValueNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
        "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
        "        self.fc3 = nn.Linear(fc2_units, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "IcwtfGQvH9BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent() :\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, gamma, baseline = True):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.gamma = gamma\n",
        "        self.baseline = baseline\n",
        "\n",
        "        self.policy = PolicyNetwork(state_size = state_size, action_size= action_size, seed= seed).to(device)\n",
        "        self.policyoptimizer = torch.optim.Adam(self.policy.parameters(), lr=LR)\n",
        "        if baseline :\n",
        "            self.value = StateValueNetwork(state_size = state_size, seed= seed).to(device)\n",
        "            self.valueoptimizer = torch.optim.Adam(self.value.parameters(), lr=LR)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        action_probs = self.policy(state)\n",
        "        state = state.detach()\n",
        "        a = Categorical(action_probs)\n",
        "        action = a.sample()\n",
        "        return action.item(), a.log_prob(action)\n",
        "\n",
        "    def update_policy(self, ret, log_probs):\n",
        "        print(torch.tensor(log_probs).shape)\n",
        "        policy_loss = -ret*torch.tensor(log_probs)\n",
        "\n",
        "        self.policyoptimizer.zero_grad()\n",
        "        torch.sum(policy_loss).backward()\n",
        "        self.policyoptimizer.step()\n",
        "\n",
        "    def update_value(self, ret, states):\n",
        "        states = torch.tensor(states).float().to(device)\n",
        "        values = self.value(states).squeeze()\n",
        "\n",
        "        val_loss = F.mse_loss(values, ret)\n",
        "        self.valueoptimizer.zero_grad()\n",
        "        val_loss.backward()\n",
        "        self.valueoptimizer.step()\n",
        "        return values"
      ],
      "metadata": {
        "id": "zQZrH4I7qYAe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a425b3-ff8b-49af-efd4-ad8e1ef76b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def reinforce(env, agent, avg_window_score, n_episodes = 10000, max_t=1000, baseline = True):\n",
        "    scores = np.zeros((n_episodes,))\n",
        "    scores_window = deque(maxlen=100)\n",
        "    for i_episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        states = []\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = agent.act(state)\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "            score += reward\n",
        "            states.append(state)\n",
        "            log_probs.append(log_prob)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "            state = new_state\n",
        "        scores[i_episode] = score\n",
        "        scores_window.append(score)\n",
        "\n",
        "\n",
        "        ret = []\n",
        "        total_reward = 0\n",
        "        for r in range(len(rewards)-1, -1, -1):\n",
        "            total_reward = r + total_reward*GAMMA\n",
        "            ret.append(total_reward)\n",
        "        ret = torch.tensor(ret[::-1]).to(device)\n",
        "        ret = (ret - ret.mean())/ret.std()\n",
        "\n",
        "        if baseline :\n",
        "            values = agent.update_value(ret, states)\n",
        "            ret = ret - values.detach()\n",
        "\n",
        "        agent.update_policy(ret, log_probs)\n",
        "\n",
        "\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=avg_window_score:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "            break\n",
        "\n",
        "    return agent, np.array(scores)"
      ],
      "metadata": {
        "id": "D6EopfueIOET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CARTPOLE"
      ],
      "metadata": {
        "id": "VkbgWN8Y4ZY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "env.seed(0)\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "no_of_actions = env.action_space.n\n",
        "\n",
        "print(state_shape)\n",
        "print(no_of_actions)\n",
        "print(env.action_space.sample())"
      ],
      "metadata": {
        "id": "bxZ84sOb8C2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 5\n",
        "episodes = 10000\n",
        "rewards_avg = np.zeros((runs, episodes))\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "for run in range(runs) :\n",
        "    s = np.random.randint(0, 1000)\n",
        "    print(\"-\"*45)\n",
        "    print(f\"Running experiment with seed value as {s}\")\n",
        "    print(\"-\"*45)\n",
        "    agent = Agent(state_size=state_shape,action_size = action_shape, seed = s, gamma = GAMMA, baseline = True)\n",
        "    agent, rewards = reinforce(env, agent,  n_episodes = episodes, avg_window_score = 195, baseline = True)\n",
        "    rewards_avg[run] = rewards"
      ],
      "metadata": {
        "id": "pQH3NEU8t4bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_run[\"with baseline\"] = rewards_avg"
      ],
      "metadata": {
        "id": "1el2oHEOebpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_rewards_episodes(steps, message = \"Rewards vs Episodes\"):\n",
        "\n",
        "  fig, ax =  plt.subplots()\n",
        "  for step in steps:\n",
        "    mini = np.mean(step, axis=0) - np.std(step, axis = 0)\n",
        "    maxi = np.mean(step, axis=0) + np.std(step, axis = 0)\n",
        "    ax.plot(np.arange(len(step[0])), np.mean(step, axis=0))\n",
        "    ax.fill_between(np.arange(len(step[0])), maxi, mini, alpha = 0.3)\n",
        "  ax.set_xlabel('Episode')\n",
        "  ax.set_ylabel('Average of Total Rewards')\n",
        "  fig.suptitle(message)\n",
        "  plt.show()\n",
        "\n",
        "plot_rewards_episodes([cartpole_run[\"with baseline\"][:, :200]])"
      ],
      "metadata": {
        "id": "zInRqgqSu2YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 5\n",
        "episodes = 10000\n",
        "rewards_avg = np.zeros((runs, episodes))\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "for run in range(runs) :\n",
        "    s = np.random.randint(0, 1000)\n",
        "    print(\"-\"*45)\n",
        "    print(f\"Running experiment with seed value as {s}\")\n",
        "    print(\"-\"*45)\n",
        "    agent = Agent(state_size=state_shape,action_size = action_shape, seed = s, gamma = GAMMA, baseline = False)\n",
        "    agent, rewards = reinforce(env, agent,  n_episodes = episodes, avg_window_score = 198, baseline = False)\n",
        "    rewards_avg[run] = rewards"
      ],
      "metadata": {
        "id": "4HMHB4QqwXzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cartpole_run[\"without baseline\"] = rewards_avg"
      ],
      "metadata": {
        "id": "zxjxvpZwwbMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards_episodes([cartpole_run[\"without baseline\"][:, :200]])"
      ],
      "metadata": {
        "id": "An6HLjDwwsqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards_episodes([cartpole_run[\"without baseline\"][:, :200], cartpole_run[\"with baseline\"][:, :200]])"
      ],
      "metadata": {
        "id": "YxEDZJxc0NIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ACROBOT"
      ],
      "metadata": {
        "id": "eHdE6Fxo3hfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('Acrobot-v1')\n",
        "env.seed(0)\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "no_of_actions = env.action_space.n\n",
        "\n",
        "print(state_shape)\n",
        "print(no_of_actions)\n",
        "print(env.action_space.sample())"
      ],
      "metadata": {
        "id": "D0M2JK3I3TKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 5\n",
        "episodes = 10000\n",
        "rewards_avg = np.zeros((runs, episodes,))\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "for run in range(runs) :\n",
        "  s = np.random.randint(0, 1000)\n",
        "  print(\"-\"*45)\n",
        "  print(f\"Running experiment with seed value as {s}\")\n",
        "  print(\"-\"*45)\n",
        "  agent = Agent(state_size=state_shape,action_size = action_shape, seed = s, gamma  = GAMMA, baseline = False)\n",
        "  agent, rewards = reinforce(env, agent, avg_window_score = -80,  n_episodes = episodes, baseline = False)\n",
        "  rewards_avg[run] = rewards"
      ],
      "metadata": {
        "id": "Mbae0BPF3i-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acrobot_run[\"without baseline\"] = rewards_avg"
      ],
      "metadata": {
        "id": "HRKwakm_8sEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = 5\n",
        "episodes = 10000\n",
        "rewards_avg = np.zeros((runs, episodes))\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "for run in range(runs) :\n",
        "    s = np.random.randint(0, 1000)\n",
        "    print(\"-\"*45)\n",
        "    print(f\"Running experiment with seed value as {s}\")\n",
        "    print(\"-\"*45)\n",
        "    agent = Agent(state_size=state_shape,action_size = action_shape, seed = s, gamma = GAMMA, baseline = True)\n",
        "    agent, rewards = reinforce(env, agent,  n_episodes = episodes, avg_window_score = -80, baseline = True)\n",
        "    rewards_avg[run] = rewards"
      ],
      "metadata": {
        "id": "oSldFgzB8uEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acrobot_run[\"with baseline\"] = rewards_avg"
      ],
      "metadata": {
        "id": "GD72T5aR80Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards_episodes([cartpole_run[\"with baseline\"][:, :1000]])"
      ],
      "metadata": {
        "id": "McxX0HPL82zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards_episodes([cartpole_run[\"without baseline\"][:, :1000]])"
      ],
      "metadata": {
        "id": "6020Pyey9AOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_rewards_episodes([cartpole_run[\"without baseline\"][:, :1000], cartpole_run[\"with baseline\"][:, :1000]])"
      ],
      "metadata": {
        "id": "w8LOMuZH9Cwm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}